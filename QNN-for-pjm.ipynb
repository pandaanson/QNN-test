{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#!conda install pytorch torchvision torchaudio -c pytorch --yes\n",
    "# Load the data\n",
    "file_path = '/home/yui/Downloads/QNN-test/pjm_east_2024_synthetic.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature target split\n",
    "target = data['value']\n",
    "features = data.drop('value', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Good old train test split\n",
    "# Convert features and target to tensors\n",
    "X = torch.tensor(features.values, dtype=torch.float32)\n",
    "y = torch.tensor(target.values, dtype=torch.float32).unsqueeze(1)  # Ensure y is the correct shape\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the scaler on your TRAINING data only\n",
    "X_train = scaler.fit_transform(X_train.numpy())  # Convert to NumPy array to fit\n",
    "X_test = scaler.transform(X_test.numpy())  # Apply the same transform to the test data\n",
    "\n",
    "# Convert scaled features back to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6364814848.0\n",
      "Epoch 2, Loss: 6415205888.0\n",
      "Epoch 3, Loss: 6291462656.0\n",
      "Epoch 4, Loss: 6314597888.0\n",
      "Epoch 5, Loss: 6131255808.0\n",
      "Epoch 6, Loss: 5665738752.0\n",
      "Epoch 7, Loss: 6200896512.0\n",
      "Epoch 8, Loss: 6032240640.0\n",
      "Epoch 9, Loss: 6243320320.0\n",
      "Epoch 10, Loss: 6124221440.0\n",
      "Epoch 11, Loss: 5662082560.0\n",
      "Epoch 12, Loss: 6196228096.0\n",
      "Epoch 13, Loss: 6352038912.0\n",
      "Epoch 14, Loss: 6006970880.0\n",
      "Epoch 15, Loss: 6249884672.0\n",
      "Epoch 16, Loss: 6360613376.0\n",
      "Epoch 17, Loss: 6407648768.0\n",
      "Epoch 18, Loss: 6347507712.0\n",
      "Epoch 19, Loss: 6797496320.0\n",
      "Epoch 20, Loss: 6206689792.0\n",
      "Epoch 21, Loss: 6032711168.0\n",
      "Epoch 22, Loss: 6304288768.0\n",
      "Epoch 23, Loss: 5556603904.0\n",
      "Epoch 24, Loss: 5939987968.0\n",
      "Epoch 25, Loss: 6186130432.0\n",
      "Epoch 26, Loss: 6089611264.0\n",
      "Epoch 27, Loss: 6429102080.0\n",
      "Epoch 28, Loss: 6134003200.0\n",
      "Epoch 29, Loss: 6332724224.0\n",
      "Epoch 30, Loss: 6097154048.0\n",
      "Epoch 31, Loss: 6354560000.0\n",
      "Epoch 32, Loss: 6327782400.0\n",
      "Epoch 33, Loss: 6304242176.0\n",
      "Epoch 34, Loss: 5821998080.0\n",
      "Epoch 35, Loss: 6051617792.0\n",
      "Epoch 36, Loss: 5916945408.0\n",
      "Epoch 37, Loss: 6143328256.0\n",
      "Epoch 38, Loss: 5831559168.0\n",
      "Epoch 39, Loss: 5995395584.0\n",
      "Epoch 40, Loss: 5995988992.0\n",
      "Epoch 41, Loss: 5941917184.0\n",
      "Epoch 42, Loss: 6567884288.0\n",
      "Epoch 43, Loss: 6231552000.0\n",
      "Epoch 44, Loss: 6592587776.0\n",
      "Epoch 45, Loss: 6088319488.0\n",
      "Epoch 46, Loss: 6431726592.0\n",
      "Epoch 47, Loss: 6187252224.0\n",
      "Epoch 48, Loss: 6235877376.0\n",
      "Epoch 49, Loss: 6040450560.0\n",
      "Epoch 50, Loss: 6379354112.0\n",
      "Epoch 51, Loss: 6284872192.0\n",
      "Epoch 52, Loss: 5880904192.0\n",
      "Epoch 53, Loss: 6100294656.0\n",
      "Epoch 54, Loss: 6225205760.0\n",
      "Epoch 55, Loss: 6310317056.0\n",
      "Epoch 56, Loss: 6123971072.0\n",
      "Epoch 57, Loss: 6507213824.0\n",
      "Epoch 58, Loss: 6417205760.0\n",
      "Epoch 59, Loss: 6287308800.0\n",
      "Epoch 60, Loss: 6055901696.0\n",
      "Epoch 61, Loss: 6165801984.0\n",
      "Epoch 62, Loss: 6082955264.0\n",
      "Epoch 63, Loss: 5820741632.0\n",
      "Epoch 64, Loss: 6093626880.0\n",
      "Epoch 65, Loss: 5931253760.0\n",
      "Epoch 66, Loss: 6164227072.0\n",
      "Epoch 67, Loss: 6139688960.0\n",
      "Epoch 68, Loss: 6062772736.0\n",
      "Epoch 69, Loss: 5627110912.0\n",
      "Epoch 70, Loss: 6195829760.0\n",
      "Epoch 71, Loss: 5755743744.0\n",
      "Epoch 72, Loss: 6402811392.0\n",
      "Epoch 73, Loss: 6288644096.0\n",
      "Epoch 74, Loss: 6046137856.0\n",
      "Epoch 75, Loss: 6481830912.0\n",
      "Epoch 76, Loss: 6035167232.0\n",
      "Epoch 77, Loss: 5877097984.0\n",
      "Epoch 78, Loss: 6281542144.0\n",
      "Epoch 79, Loss: 6374635520.0\n",
      "Epoch 80, Loss: 6449698816.0\n",
      "Epoch 81, Loss: 6404021760.0\n",
      "Epoch 82, Loss: 5768958464.0\n",
      "Epoch 83, Loss: 6046268928.0\n",
      "Epoch 84, Loss: 5685312000.0\n",
      "Epoch 85, Loss: 5847379968.0\n",
      "Epoch 86, Loss: 6287432704.0\n",
      "Epoch 87, Loss: 6001911296.0\n",
      "Epoch 88, Loss: 6372971008.0\n",
      "Epoch 89, Loss: 6160171520.0\n",
      "Epoch 90, Loss: 5911967744.0\n",
      "Epoch 91, Loss: 6091350528.0\n",
      "Epoch 92, Loss: 5794827264.0\n",
      "Epoch 93, Loss: 6097040896.0\n",
      "Epoch 94, Loss: 6366499840.0\n",
      "Epoch 95, Loss: 6226395648.0\n",
      "Epoch 96, Loss: 5813716992.0\n",
      "Epoch 97, Loss: 6106441216.0\n",
      "Epoch 98, Loss: 6195572736.0\n",
      "Epoch 99, Loss: 6127155200.0\n",
      "Epoch 100, Loss: 5911083520.0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class QuantileNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_quantiles):\n",
    "        super(QuantileNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_quantiles)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def quantile_loss(preds, target, quantiles):\n",
    "    assert len(quantiles) == preds.shape[1], \"Quantiles size must match predictions width.\"\n",
    "    errors = target.unsqueeze(1) - preds\n",
    "    return torch.max((quantiles - 1) * errors, quantiles * errors).mean()\n",
    "\n",
    "# Initialize the model\n",
    "num_features = X_train.shape[1]\n",
    "num_quantiles = 50\n",
    "quantiles = torch.linspace(0.01, 0.99, steps=num_quantiles)\n",
    "model = QuantileNetwork(input_size=num_features, num_quantiles=num_quantiles)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_x)\n",
    "        loss = quantile_loss(preds, batch_y, quantiles)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Configuration So far: (128, (32, 64, 128), 0.0, 'Adam', 9.999999999999999e-06) with Loss: 6175512160.0\n",
      "Search done for 128,(32, 64, 128),0.0,Adam,9.999999999999999e-06\n",
      "Completed: 0.28%\n",
      "Completed: 0.56%\n",
      "Best Configuration So far: (128, (32, 64, 128), 0.0, 'Adam', 0.001) with Loss: 2681570368.0\n",
      "Search done for 128,(32, 64, 128),0.0,Adam,0.001\n",
      "Completed: 0.83%\n",
      "Best Configuration So far: (128, (32, 64, 128), 0.0, 'Adam', 0.01) with Loss: 1069739308.0\n",
      "Search done for 128,(32, 64, 128),0.0,Adam,0.01\n",
      "Completed: 1.11%\n",
      "Best Configuration So far: (128, (32, 64, 128), 0.0, 'Adam', 0.09999999999999999) with Loss: 1032999016.0\n",
      "Search done for 128,(32, 64, 128),0.0,Adam,0.09999999999999999\n",
      "Completed: 1.39%\n",
      "Completed: 1.67%\n",
      "Completed: 1.94%\n",
      "Completed: 2.22%\n",
      "Completed: 2.50%\n",
      "Completed: 2.78%\n",
      "Completed: 3.06%\n",
      "Completed: 3.33%\n",
      "Completed: 3.61%\n",
      "Completed: 3.89%\n",
      "Completed: 4.17%\n",
      "Completed: 4.44%\n",
      "Completed: 4.72%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "num_features = X_train.shape[1]\n",
    "num_quantiles = 50\n",
    "quantiles = torch.linspace(0.01, 0.99, steps=num_quantiles)\n",
    "def quantile_loss(preds, target, quantiles):\n",
    "    assert len(quantiles) == preds.shape[1], \"Quantiles size must match predictions width.\"\n",
    "    errors = target.unsqueeze(1) - preds\n",
    "    return torch.max((quantiles - 1) * errors, quantiles * errors).mean()\n",
    "# Define Quantile Network with three hidden layers\n",
    "class QuantileNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, dropout_rate):\n",
    "        super(QuantileNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, hidden_sizes[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))  # Output layer\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Train and evaluate model\n",
    "def train_and_evaluate(model, train_loader, val_loader, optimizer, quantiles, epochs=2000):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = quantile_loss(output, target, quantiles)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            loss = quantile_loss(output, target, quantiles)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Hyperparameter search space adjustments\n",
    "neurons_options = [\n",
    "    (32, 64, 128),\n",
    "    (64, 128, 256),\n",
    "    (128, 256, 512)\n",
    "]\n",
    "batch_sizes = [128, 512]#[64, 128, 256, 512]\n",
    "neurons = [32, 64, 128, 256]\n",
    "optimizers_dict = {'Adam': optim.Adam, 'Adagrad': optim.Adagrad, 'RMSprop': optim.RMSprop}\n",
    "dropout_rates = np.linspace(0, 0.3, num=4)\n",
    "learning_rates = np.logspace(-5, -1, num=5)\n",
    "# Calculate the total number of iterations\n",
    "total_iterations = len(batch_sizes) * len(neurons_options) * len(dropout_rates) * len(optimizers_dict) * len(learning_rates)\n",
    "current_iteration = 0\n",
    "# Hyperparameter search\n",
    "best_loss = np.inf\n",
    "best_config = None\n",
    "best_model=None\n",
    "for batch_size in batch_sizes:\n",
    "    for hidden_sizes in neurons_options:  # Corrected from neurons to neurons_options\n",
    "        for dropout_rate in dropout_rates:\n",
    "            for optimizer_name, optimizer_class in optimizers_dict.items():\n",
    "                for lr in learning_rates:\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    # Correct instantiation with the actual input size\n",
    "                    model = QuantileNetwork(input_size=85, output_size=50, hidden_sizes=hidden_sizes, dropout_rate=dropout_rate)\n",
    "                    optimizer = optimizer_class(model.parameters(), lr=lr)  # Corrected from opt to optimizer\n",
    "                    loss = train_and_evaluate(model, train_loader, val_loader, optimizer, quantiles, epochs=100)  # Specified epochs for clarity\n",
    "                    if loss < best_loss:\n",
    "                        best_loss = loss\n",
    "                        best_config = (batch_size, hidden_sizes, dropout_rate, optimizer_name, lr)\n",
    "                        best_model = model\n",
    "                        print(f\"Best Configuration So far: {best_config} with Loss: {best_loss}\")\n",
    "                        print(f\"Search done for {batch_size},{hidden_sizes},{dropout_rate},{optimizer_name},{lr}\")\n",
    "                    current_iteration += 1\n",
    "                    completion_percentage = (current_iteration / total_iterations) * 100\n",
    "                    print(f\"Completed: {completion_percentage:.2f}%\")\n",
    "\n",
    "print(f\"Best Configuration: {best_config} with Loss: {best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "def evaluate_and_plot_quantiles_direct(model, loader, quantiles):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Collect predictions and targets\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = model(data)\n",
    "            all_preds.append(output)\n",
    "            all_targets.append(target)\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    # Initialize counters for each quantile\n",
    "    quantile_counts = [0] * (len(quantiles) + 1)  # Including one extra for above highest quantile\n",
    "\n",
    "    # Assign each target to the highest quantile it does not exceed\n",
    "    for i, target in enumerate(all_targets):\n",
    "        # Find the highest quantile that the target value does not exceed\n",
    "        for j in range(len(quantiles)):\n",
    "            if target <= all_preds[i, j]:\n",
    "                quantile_counts[j] += 1\n",
    "                break\n",
    "        else:\n",
    "            # If none of the quantiles is greater than the target, it belongs to the last category\n",
    "            quantile_counts[-1] += 1\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_samples = len(all_targets)\n",
    "    percentages = [count / total_samples * 100 for count in quantile_counts]\n",
    "\n",
    "    # Plotting the results for visual confirmation\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(percentages)), percentages, color='skyblue')\n",
    "    plt.xticks(ticks=range(len(percentages)), labels=[f'{q:.2f}' for q in quantiles] + ['>0.99'], rotation=45)\n",
    "    plt.xlabel('Quantile Ranges')\n",
    "    plt.ylabel('Percentage of Data Points')\n",
    "    plt.title('Distribution of Data Points Across Quantiles')\n",
    "    plt.show()\n",
    "\n",
    "    return percentages\n",
    "\n",
    "# Evaluate and visualize the distribution of data points across quantiles\n",
    "percentages = evaluate_and_plot_quantiles_direct(best_model, val_loader, quantiles)\n",
    "\n",
    "# Print the results\n",
    "for i, pct in enumerate(percentages):\n",
    "    if i < len(quantiles):\n",
    "        print(f\"Percentage of data <= {quantiles[i]:.2f} quantile: {pct:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Percentage of data > {quantiles[-1]:.2f} quantile: {pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_selected_quantile_predictions_with_actuals(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Collect all predictions and actual targets\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = model(data)\n",
    "            all_preds.append(output.numpy())  # Assuming the data fits in memory\n",
    "            all_targets.append(target.numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    quantiles_np = np.linspace(0.01, 0.99, num=all_preds.shape[1])\n",
    "\n",
    "    # Select indices for lowest, middle, and highest quantile\n",
    "    lowest_quantile_index = 0  # First quantile\n",
    "    middle_quantile_index = all_preds.shape[1] // 2  # Middle quantile\n",
    "    highest_quantile_index = all_preds.shape[1] - 1  # Last quantile\n",
    "\n",
    "    # Create an x-axis for the data points\n",
    "    x_axis = np.arange(all_preds.shape[0])\n",
    "\n",
    "    # Plotting selected quantile predictions\n",
    "    plt.plot(x_axis, all_preds[:, lowest_quantile_index], label=f'Lowest Quantile {quantiles_np[lowest_quantile_index]:.2f}', color='blue')\n",
    "    plt.plot(x_axis, all_preds[:, middle_quantile_index], label=f'Middle Quantile {quantiles_np[middle_quantile_index]:.2f}', color='green')\n",
    "    plt.plot(x_axis, all_preds[:, highest_quantile_index], label=f'Highest Quantile {quantiles_np[highest_quantile_index]:.2f}', color='purple')\n",
    "\n",
    "    # Overlay actual data points as scatter plot\n",
    "    plt.scatter(x_axis, all_targets, color='red', alpha=0.5, label='Actual Data', s=10)\n",
    "\n",
    "    plt.title('Selected Quantile Predictions and Actual Data Points Indexed by Data Order')\n",
    "    plt.xlabel('Index of Data Point')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'val_loader' is already defined and loaded with the appropriate data\n",
    "plot_selected_quantile_predictions_with_actuals(best_model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in DataLoader(TensorDataset(X_test, y_test), batch_size=64):\n",
    "        preds = best_model(X_batch)\n",
    "        loss = quantile_loss(preds, y_batch, quantiles)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f'Test Loss: {test_loss / len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quantiles_individual(model, loader, quantiles):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Collect predictions and actual targets\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = model(data)\n",
    "            all_preds.append(output)\n",
    "            all_targets.append(target)\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    # Initialize quantile count\n",
    "    quantile_counts = [0] * (len(quantiles) + 1)  # +1 for targets above the highest quantile\n",
    "\n",
    "    # Assign each target to a quantile\n",
    "    for target in all_targets:\n",
    "        # Determine which quantile range each target belongs to\n",
    "        # Compare target against all predictions and count how many quantiles it is greater than\n",
    "        quantile_index = (target > all_preds).long().sum()  # How many quantiles are less than the target\n",
    "        quantile_index = min(quantile_index, len(quantiles))  # Clamp to the number of quantiles to handle edge cases\n",
    "        quantile_counts[quantile_index] += 1\n",
    "\n",
    "    total_samples = float(all_targets.shape[0])\n",
    "    percentages = [count / total_samples * 100 for count in quantile_counts]\n",
    "\n",
    "    return percentages\n",
    "\n",
    "# Evaluate and print the results\n",
    "percentages = evaluate_quantiles_individual(best_model, val_loader, quantiles)\n",
    "\n",
    "# Print results for each quantile\n",
    "for i, pct in enumerate(percentages[:-1]):\n",
    "    print(f\"Percentage of data within the {quantiles[i] if i < len(quantiles) else 'last'} quantile: {pct:.2f}%\")\n",
    "if percentages:\n",
    "    print(f\"Percentage of data above the highest quantile {quantiles[-1]:.2f}: {percentages[-1]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_quantiles_with_overlap_check(model, loader, quantiles):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = model(data)\n",
    "            all_preds.append(output)\n",
    "            all_targets.append(target)\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    sorted_preds = all_preds.sort(dim=1).values\n",
    "\n",
    "    # Check and print overlap\n",
    "    overlaps = (sorted_preds[:, 1:] <= sorted_preds[:, :-1]).sum()\n",
    "    if overlaps.item() > 0:\n",
    "        print(f\"Overlap detected in quantile predictions: {overlaps.item()} instances\")\n",
    "\n",
    "    quantile_counts = [0] * (sorted_preds.shape[1] - 1)\n",
    "    for i in range(sorted_preds.shape[1] - 1):\n",
    "        cond = (all_targets >= sorted_preds[:, i]) & (all_targets < sorted_preds[:, i + 1])\n",
    "        quantile_counts[i] = cond.sum().item()\n",
    "\n",
    "    total_samples = float(all_targets.shape[0])\n",
    "    percentages = [count / total_samples * 100 for count in quantile_counts]\n",
    "    total_percentage = sum(percentages)\n",
    "    if abs(100.0 - total_percentage) > 0.1:\n",
    "        print(f\"Warning: Total percentage does not sum to 100% but is {total_percentage}%\")\n",
    "\n",
    "    return percentages\n",
    "\n",
    "# Evaluate and check for overlaps\n",
    "percentages = evaluate_quantiles_with_overlap_check(best_model, val_loader, quantiles)\n",
    "\n",
    "# Print results\n",
    "quantiles_np = quantiles.numpy()\n",
    "for i in range(len(percentages)):\n",
    "    print(f\"Percentage of data between {quantiles_np[i]:.2f} and {quantiles_np[i+1]:.2f} quantiles: {percentages[i]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "def evaluate_and_plot_quantiles(model, loader, quantiles):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Collect predictions and targets\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = model(data)  # Assuming this is outputting the quantiles directly\n",
    "            all_preds.append(output)\n",
    "            all_targets.append(target)\n",
    "\n",
    "    # Convert collected data to single tensors\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    # Sorting predictions along quantiles for each sample\n",
    "    sorted_preds = all_preds.sort(dim=1).values\n",
    "\n",
    "    # Plotting the quantile distributions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    quantiles_np = quantiles.numpy()  # Assuming quantiles is a torch tensor\n",
    "    for i in range(sorted_preds.shape[1]):\n",
    "        sns.kdeplot(sorted_preds[:, i].numpy(), label=f'Quantile {quantiles_np[i]:.2f}')\n",
    "    plt.title('Distribution of Each Quantile Prediction')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Check for overlaps\n",
    "    overlaps = (sorted_preds[:, 1:] <= sorted_preds[:, :-1]).sum()\n",
    "    if overlaps.item() > 0:\n",
    "        print(f\"Overlap detected in quantile predictions: {overlaps.item()} instances\")\n",
    "\n",
    "    # Calculating percentages\n",
    "    quantile_counts = [0] * (sorted_preds.shape[1] - 1)\n",
    "    for i in range(sorted_preds.shape[1] - 1):\n",
    "        cond = (all_targets >= sorted_preds[:, i]) & (all_targets < sorted_preds[:, i + 1])\n",
    "        quantile_counts[i] = cond.sum().item()\n",
    "\n",
    "    total_samples = float(all_targets.shape[0])\n",
    "    percentages = [count / total_samples * 100 for count in quantile_counts]\n",
    "    total_percentage = sum(percentages)\n",
    "    if abs(100.0 - total_percentage) > 0.1:\n",
    "        print(f\"Warning: Total percentage does not sum to 100% but is {total_percentage}%\")\n",
    "\n",
    "    return percentages\n",
    "\n",
    "# Using the model, test loader and the defined function to evaluate and plot\n",
    "percentages = evaluate_and_plot_quantiles(best_model, val_loader, quantiles)\n",
    "\n",
    "# Printing percentages\n",
    "for i in range(len(percentages)):\n",
    "    print(f\"Percentage of data between {quantiles[i]:.2f} and {quantiles[i+1]:.2f} quantiles: {percentages[i]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def plot_quantile_ranges(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "\n",
    "    # Collect all predictions\n",
    "    with torch.no_grad():\n",
    "        for data, _ in loader:\n",
    "            output = model(data)  # Assuming this is outputting the quantiles directly\n",
    "            all_preds.append(output)\n",
    "\n",
    "    # Convert collected data to a single tensor\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "\n",
    "    # Sorting predictions along quantiles for each sample\n",
    "    sorted_preds = all_preds.sort(dim=1).values\n",
    "\n",
    "    # Create an x-axis for the data points\n",
    "    x_axis = np.arange(sorted_preds.shape[0])\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    quantiles_np = np.linspace(0.01, 0.99, num=sorted_preds.shape[1])\n",
    "    for i in range(sorted_preds.shape[1]):\n",
    "        plt.plot(x_axis, sorted_preds[:, i].numpy(), label=f'Quantile {quantiles_np[i]:.2f}')\n",
    "\n",
    "    plt.title('Quantile Predictions Across Data Index')\n",
    "    plt.xlabel('Index of Data Point')\n",
    "    plt.ylabel('Quantile Predicted Value')\n",
    "    plt.legend(title='Quantiles', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'val_loader' is already defined and loaded with the appropriate data\n",
    "plot_quantile_ranges(best_model, val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
