{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#!conda install pytorch torchvision torchaudio -c pytorch --yes\n",
    "# Load the data\n",
    "file_path = '/home/yui/Downloads/QNN-test/pjm_east_2024_synthetic.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature target split\n",
    "target = data['value']\n",
    "features = data.drop('value', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Good old train test split\n",
    "# Convert features and target to tensors\n",
    "X = torch.tensor(features.values, dtype=torch.float32)\n",
    "y = torch.tensor(target.values, dtype=torch.float32).unsqueeze(1)  # Ensure y is the correct shape\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the scaler on your TRAINING data only\n",
    "X_train = scaler.fit_transform(X_train.numpy())  # Convert to NumPy array to fit\n",
    "X_test = scaler.transform(X_test.numpy())  # Apply the same transform to the test data\n",
    "\n",
    "# Convert scaled features back to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6107223040.0\n",
      "Epoch 2, Loss: 5959763968.0\n",
      "Epoch 3, Loss: 6639678976.0\n",
      "Epoch 4, Loss: 6632449024.0\n",
      "Epoch 5, Loss: 6164440576.0\n",
      "Epoch 6, Loss: 6239736320.0\n",
      "Epoch 7, Loss: 6447383040.0\n",
      "Epoch 8, Loss: 5980840448.0\n",
      "Epoch 9, Loss: 5850449920.0\n",
      "Epoch 10, Loss: 6090036224.0\n",
      "Epoch 11, Loss: 6178769920.0\n",
      "Epoch 12, Loss: 6312083968.0\n",
      "Epoch 13, Loss: 6012160000.0\n",
      "Epoch 14, Loss: 6531036160.0\n",
      "Epoch 15, Loss: 5982267392.0\n",
      "Epoch 16, Loss: 6337232896.0\n",
      "Epoch 17, Loss: 6204045312.0\n",
      "Epoch 18, Loss: 6371545088.0\n",
      "Epoch 19, Loss: 6026193920.0\n",
      "Epoch 20, Loss: 6213103104.0\n",
      "Epoch 21, Loss: 6172084736.0\n",
      "Epoch 22, Loss: 6259755520.0\n",
      "Epoch 23, Loss: 6323193344.0\n",
      "Epoch 24, Loss: 5906226176.0\n",
      "Epoch 25, Loss: 6454215168.0\n",
      "Epoch 26, Loss: 6191072768.0\n",
      "Epoch 27, Loss: 6149027840.0\n",
      "Epoch 28, Loss: 6036398592.0\n",
      "Epoch 29, Loss: 6285844992.0\n",
      "Epoch 30, Loss: 6244517376.0\n",
      "Epoch 31, Loss: 6196805632.0\n",
      "Epoch 32, Loss: 6431005696.0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class QuantileNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_quantiles):\n",
    "        super(QuantileNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_quantiles)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def quantile_loss(preds, target, quantiles):\n",
    "    assert len(quantiles) == preds.shape[1], \"Quantiles size must match predictions width.\"\n",
    "    errors = target.unsqueeze(1) - preds\n",
    "    return torch.max((quantiles - 1) * errors, quantiles * errors).mean()\n",
    "\n",
    "# Initialize the model\n",
    "num_features = X_train.shape[1]\n",
    "num_quantiles = 50\n",
    "quantiles = torch.linspace(0.01, 0.99, steps=num_quantiles)\n",
    "model = QuantileNetwork(input_size=num_features, num_quantiles=num_quantiles)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_x)\n",
    "        loss = quantile_loss(preds, batch_y, quantiles)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "num_features = X_train.shape[1]\n",
    "num_quantiles = 50\n",
    "quantiles = torch.linspace(0.01, 0.99, steps=num_quantiles)\n",
    "def quantile_loss(preds, target, quantiles):\n",
    "    assert len(quantiles) == preds.shape[1], \"Quantiles size must match predictions width.\"\n",
    "    errors = target.unsqueeze(1) - preds\n",
    "    return torch.max((quantiles - 1) * errors, quantiles * errors).mean()\n",
    "# Define Quantile Network with three hidden layers\n",
    "class QuantileNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, dropout_rate):\n",
    "        super(QuantileNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, hidden_sizes[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))  # Output layer\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Train and evaluate model\n",
    "def train_and_evaluate(model, train_loader, val_loader, optimizer, quantiles, epochs=2000):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = quantile_loss(output, target, quantiles)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            loss = quantile_loss(output, target, quantiles)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Hyperparameter search space adjustments\n",
    "neurons_options = [\n",
    "    (32, 64, 128),\n",
    "    (64, 128, 256),\n",
    "    (128, 256, 512)\n",
    "]\n",
    "batch_sizes = [128, 512]#[64, 128, 256, 512]\n",
    "neurons = [32, 64, 128, 256]\n",
    "optimizers_dict = {'Adam': optim.Adam, 'Adagrad': optim.Adagrad, 'RMSprop': optim.RMSprop}\n",
    "dropout_rates = np.linspace(0, 0.3, num=4)\n",
    "learning_rates = np.logspace(-5, -1, num=5)\n",
    "# Calculate the total number of iterations\n",
    "total_iterations = len(batch_sizes) * len(neurons_options) * len(dropout_rates) * len(optimizers_dict) * len(learning_rates)\n",
    "current_iteration = 0\n",
    "# Hyperparameter search\n",
    "best_loss = np.inf\n",
    "best_config = None\n",
    "best_model=None\n",
    "for batch_size in batch_sizes:\n",
    "    for hidden_sizes in neurons_options:  # Corrected from neurons to neurons_options\n",
    "        for dropout_rate in dropout_rates:\n",
    "            for optimizer_name, optimizer_class in optimizers_dict.items():\n",
    "                for lr in learning_rates:\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    # Correct instantiation with the actual input size\n",
    "                    model = QuantileNetwork(input_size=85, output_size=50, hidden_sizes=hidden_sizes, dropout_rate=dropout_rate)\n",
    "                    optimizer = optimizer_class(model.parameters(), lr=lr)  # Corrected from opt to optimizer\n",
    "                    loss = train_and_evaluate(model, train_loader, val_loader, optimizer, quantiles, epochs=100)  # Specified epochs for clarity\n",
    "                    if loss < best_loss:\n",
    "                        best_loss = loss\n",
    "                        best_config = (batch_size, hidden_sizes, dropout_rate, optimizer_name, lr)\n",
    "                        best_model = model\n",
    "                        print(f\"Best Configuration So far: {best_config} with Loss: {best_loss}\")\n",
    "                        print(f\"Search done for {batch_size},{hidden_sizes},{dropout_rate},{optimizer_name},{lr}\")\n",
    "                    current_iteration += 1\n",
    "                    completion_percentage = (current_iteration / total_iterations) * 100\n",
    "                    print(f\"Completed: {completion_percentage:.2f}%\")\n",
    "\n",
    "print(f\"Best Configuration: {best_config} with Loss: {best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
